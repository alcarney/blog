[{"content":"It\u0026rsquo;s possible to create custom Python environments for use within a Jupyter notebook without having to run a jupyter server from each of them. The following steps will allow you to have a single jupyter server running and have it use a variety of Python environments in its notebooks\n Create a virtualenv and install any packages that you want available pip install ipykernel ipykernel install --user --name \u0026lt;envname\u0026gt; --display-name \u0026lt;display name\u0026gt;  Then the new environment should become available in the Change kernel menu\n","id":"custom-jupyter-kernels","tags":["python","jupyter"],"title":"Custom Jupyter Kernels"},{"content":"Jackson can automatically detect the relevant abstract class to initialise when parsing some JSON if you give it a little bit of help. There are multiple methods Jackson can use to detect and indicate the class it should use but the method I have chosen is to tell it to look for a type field.\nConsider the following json snippets\n{ \u0026#34;id\u0026#34;: \u0026#34;123\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;name\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Alex\u0026#34; } { \u0026#34;id\u0026#34;: \u0026#34;345\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;value\u0026#34;, \u0026#34;value\u0026#34;: 123.4 } Which can be represented by the following classes\npublic class IdName extends IdField { public static final String FIELD_TYPE = \u0026#34;name\u0026#34;; private String name; public IdName() { super(FIELD_TYPE); } ... } public class IdValue extends IdField { public static final String FIELD_TYPE = \u0026#34;value\u0026#34;; private float value; public IdValue() { super(FIELD_TYPE); } ... } Then to enable Jackson to determine which subclass to initialise we need to add some annotations to the base class definition\n@JsonTypeInfo(use = JsonTypeInfo.Id.NAME, include = JsonTypeInfo.As.EXISTING_PROPERTY, property = \u0026#34;type\u0026#34;) @JsonSubTypes({ @JsonSubTypes.Type(value = IdName.class, name = IdName.FIELD_TYPE), @JsonSubTypes.Type(value = IdValue.class, name = IdValue.FIELD_TYPE) }) public abstract class IdField { private String id; private String type; public IdField(String type) { this.id = UUID.randomUUID().toString(); this.type = type; } ... } Then we can just parse the json as we normally would.\nString jsonStr = \u0026#34;{\\\u0026#34;id\\\u0026#34;:\\\u0026#34;123\\\u0026#34;,\\\u0026#34;name\\\u0026#34;:\\\u0026#34;Alex\\\u0026#34;}\u0026#34;; ObjectMapper mapper = new ObjectMapper(); IdField field = mapper.readValue(jsonStr, IdField.class); System.out.println(field.getType()); // =\u0026gt; \u0026#34;name\u0026#34;  IdName nameField = (IdName) field; System.out.println(nameField.getName()); // =\u0026gt; \u0026#34;Alex\u0026#34; ","id":"deserialising-abstract-classes-with-jackson","tags":["java","json"],"title":"Deserialising Abstract Classes with Jackson"},{"content":"Something that\u0026rsquo;s quite useful in Emacs is the ability to start a running process in the background. This can be done with the start-processfunction.\nExamples We can set the directory to run in by wrapping the call to start-process in a let expression that sets the default-directory variable.\n(let ((default-directory \u0026#34;~/Projects/blog\u0026#34;)) (start-process \u0026#34;hugo\u0026#34; \u0026#34;hugo[blog]\u0026#34; \u0026#34;hugo\u0026#34; \u0026#34;server\u0026#34; \u0026#34;--buildDrafts\u0026#34; \u0026#34;--buildFuture\u0026#34;)) There\u0026rsquo;s a lot of mentions of hugo here let\u0026rsquo;s break this down a bit\n The first gives a name to the process, this will show up in places like the process-menu The second names the buffer that will be used to display the output from the process The third is the hugo command itself, followed by any additional arguments to pass to it.  ","id":"emacs-sub-processes","tags":["emacs"],"title":"Emacs Sub Processes"},{"content":"Turns out this was rather easy thanks to libinput-gestures\nInstallation Using arch linux this process was made nice and simple thanks to the AUR package\n$ yaourt -S libinput-gestures Configuration libinput-gestures comes with its own configuration file /etc/libinput-gestures.conf that\u0026rsquo;s well documented and has a straightforward syntax. Below is my current configuration\ngesture swipe up xdotool key ctrl+F9 gesture swipe down xdotool key alt+space gesture swipe left _internal ws_down gesture swipe right _internal ws_up Each gesture works with either 3 or 4 fingers and executes the command on the right when triggered. Each direction is mapped to the following command.\n up: Sends the key combination ctrl+F9 which in KDE presents all the windows on the current desktop down: Sends the key combination alt+space which I have set KDE to open Plasma Search left: Go to the previous workspace right: Go to the next workspace  ","id":"enabling-touchpad-gestures-on-linux","tags":["linux"],"title":"Enabling Touchpad Gestures on Linux"},{"content":"  Create a folder for global packages to be install into\n$ mdkir \u0026#34;${HOME}/.npm-packages\u0026#34;    Tell npm to use this folder\n$ npm config set prefix \u0026#34;${HOME}/.npm-packages\u0026#34;    Add the corresponding ${HOME}/.npm-packages/bin folder to your PATH\n  ","id":"installing-npm-packages-globally-without-root","tags":["npm","js"],"title":"Installing npm packages globally without root"},{"content":"Queries A collection of queries for inspecting data stored in a Prometheus server. Most of these queries should work in vanilla prometheus however a small number may contain a few \u0026ldquo;grafana-isms\u0026rdquo;\nThe following variables are used as examples\n   Name Description     $interval A time interval e.g. 1m. 5m, 1h etc.    Nodes Queries relating to the state of the \u0026ldquo;physical\u0026rdquo; hardware that is hosting the cluster\nThe following metrics are used in the exmaples\n   Name Description     node_cpu_seconds_total CPU time broken down into modes e.g. idle, system, user   node_load1/5/15 1m, 5m, 15m CPU load averages.    CPU\nCPU Utilisation, broken down by node\nsum(rate(node_cpu_seconds_total{mode!=\u0026quot;idle\u0026quot;, mode!=\u0026quot;iowait\u0026quot;}[$interval])) by (instance) CPU load times, broken down by node normalised by number of cores\nsum(node_load1) by (instance) / count(node_cpu_seconds_total{mode=\u0026quot;system\u0026quot;}) by (instance) * 100 Kubernetes A collection of queries specific to kubernetes instances\nPods\nUseful for alerts, this query returns which pods are not ready\nsum(kube_pod_container_status_ready) by (pod) \u0026lt; 1 This returns the number of times a pod has restarted\nkube_pod_container_status_restarts_total Useful for alerts, this query will return the pods that are waiting and the reason for the delay\nsum(kube_pod_container_status_waiting_reason) by (pod, reason) \u0026gt; 0 Services\nA collection of queries for metrics useful for monitoring services\nThe following metrics will be used as examples\n   Name Description     service_requests A counter that counts the number of requests received   service_requests_latency A histogram that records the response times for a request   service_info A counter that encodes information about the service in the metric labels    The number of requests received (usually per second)\nsum(rate(service_requests[$interval])) The number of internal server errors (5xx responses), again usually per second\nsum(rate(service_requests{code=~\u0026quot;5.*\u0026quot;}[$interval])) The number of user errors (4xx responses), per second\nsum(rate(service_requests{code=~\u0026quot;4.*\u0026quot;}[$interval])) Each of the by can be broken down by service (assuming the existence of a service label in the scraped data) as follows\nsum(rate(fdm_requests[$interval])) by (service) Request latencies are recorded as a histogram and so we can only collect aggregate values. As far as I can tell the usual way to do this is to generate the following datapoints\n 95th/99th percentile 50th percentile / median average response time  The Xth percentile tells us the upper bound on the amount of time X% requests are processed in. E.g. if the 95th percentile is 500ms, then 95% of requests are handled in 500ms or less.\nUsing these metrics we can get a feel for the distribution of the request times:\n If the median coincides with the mean then we can infer that the response times are normally distributed If the mean \u0026lt; median then the distribution is skewed towards zero i.e. the majority of requests are being processed quicker If the mean \u0026gt; median then the distribution is skewed high, i.e. the majority of requests take a longer time to be processed.  To generate the Xth percentile\nhistogram_quantile(0.X, sum(rate(service_requests_latency_bucket[$interval])) by (le)) NOTE: This will only work if the bucket label is called le\nTo get the average\nsum(rate(service_requests_latency_sum[$interval])) / sum(rate(service_requests_latency_count[$interval])) To get a count of the number of services\ncount(sum(service_info) by (service)) Assuming the existence of a service label\n","id":"kubernetes-focused-prometheus-queries","tags":["monitoring","devops","prometheus","k8s"],"title":"Kubernetes Focused Prometheus Queries"},{"content":"","id":"notes","tags":null,"title":"Notes"},{"content":"Submodules To clone a repo that contains submodules we can run the following command\n$ git clone --recurse-submodules \u0026lt;repo-url\u0026gt; Or if you\u0026rsquo;ve already cloned a repo only to later discover that it contained submodules\n$ git submodule update --init --recursive ","id":"notes-on-git","tags":["git"],"title":"Notes on Git"},{"content":"While at the time of writing (May 2019) the death of Python 2 is just around the corner since I\u0026rsquo;m working in an enterprise environment I\u0026rsquo;m sure I will be dealing with Python 2.x code for some time to come. To that end here are some gotchas to keep in mind.\nMissing Features In the 10+ years since Python 3\u0026rsquo;s release it\u0026rsquo;s not surprising that it has accumlated a large list of features that simply don\u0026rsquo;t exist in Python 2.x\nNo enum module The enum module was added in Python 3.4. In order to make use of this module in versions of Python older than this we can install the enum34 package from PyPi.\n$ pip install enum34 Code that uses the standard enum module should now work as is.\nNote: It would seem that this package is not actively maintained with the most recent change being in 2016 at the time of writing.\nNo pathlib module The pathlib module was added in Python 3.4. In order to make use of this module in older versions of Python we can install the pathlib2 package on PyPi\n$ pip install pathlib2 Then code written for the standard pathlib module should work as expected.\nNo FileNotFoundErrors Not sure when these were added. If you simply want to be able to throw one it\u0026rsquo;s easy enough to backport one yourself\nclass FileNotFoundError(OSError): pass Moved / Changed Features There are a number of features that did exist in Python 2.x but were renamed/moved or tweaked in someway in Python 3.x so that they are not directly compatible with older interpreters anymore\nconfigparser is ConfigParser Before Python 3 the configparser module was called ConfigParser, I\u0026rsquo;m struggling to see if there are any functional differences between the two versions but there is a actively maintained configparser package available which has apparently backported a number of changes to older versions of Python\n$ pip install configparser In theory code using the standard configparser module will \u0026ldquo;just work\u0026rdquo;.\nBest Practice Here is a miscellaneous collection things to do when writing code that will run under Python 2.x that should hopefully minimise the amount of surprises!\nUse new style classes In Python 2.x new style classes are an explicit opt-in\nclass MyClass(object): ... Define both __eq__ and __ne__ When creating a class that you wish to define equality for you need to ensure that you define both the __eq__ and __ne__ special methods for your class to have the behavior you would expect it to.\nIn Python 2.x the default implementation of __ne__ is something like the following.\ndef __ne__(self, other): return not (self is other) For your class to behave as expected you will need to define it as follows\nclass MyClass(object): ... def __eq__(self, other): ... def __ne__(self, other): return not self.__eq__(other) str != str In Python 3 the string related datatypes were overhauled to include built-in support for unicode character encodings. This means that the str type in Python 2.x is not equivalent to the str type in Python 3.x\nSo if you want to check that some value is an instance of the type str the best way is to make use the six compatibility package\n$ pip install six And to use the following code\nimport six if isinstance(value, six.string_types): ... ","id":"python-2.x-gotchas","tags":["python"],"title":"Python 2.x Gotchas"},{"content":"Beware pathlib on 3.5 While pathlib exists in Python 3.5, it\u0026rsquo;s not fully integrated yet. Passing a pathlib.Path object to the built-in open method will result in a surprising error\nTypeError: invalid file: PosixPath(\u0026#39;path/to/file.txt\u0026#39;) In this situation it\u0026rsquo;s better to call the path\u0026rsquo;s open() method instead.\n","id":"python-3.x-gotchas","tags":["python"],"title":"Python 3.x Gotchas"},{"content":"Pattern Rules A pattern rule can be used to define a generic recipe for turning a file of type X into a file a type Y for example, compiling program.c into program.o. A pattern rule can be defined as follows\n%.o: %.c $(CC) -c $(CFLAGS) $\u0026lt; -o $@  %.o/%.c Will match files of the form *.o and *.c respectively $\u0026lt; can be used to reference all the dependencies of the target, in this case the *.c file. $@ can be used to reference the target itself, in this case the $@ file  Example .POSIX: CC = gcc CFLAGS = -Wall $(shell pkg-config --cflags xcb-image) LDLIBS = $(shell pkg-config --libs xcb-image) default: main debug: CFLAGS += -g debug: main main: main.o $(CC) $\u0026lt; -o $@ $(LDLIBS) %.o: %.c $(CC) -c $(CFLAGS) $\u0026lt; -o $@ ","id":"writing-makefiles","tags":["c","make"],"title":"Writing Makefiles"}]